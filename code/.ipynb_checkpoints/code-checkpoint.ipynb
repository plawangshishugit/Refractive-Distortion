{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebe463-ebe3-4e80-93a1-4764d600c580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9720b80b-499b-4d1f-941b-1d7927146d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp39-cp39-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Using cached matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "Using cached contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.6 MB/s eta 0:00:00\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.56.0 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pyparsing-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c853af6-1577-45e7-8087-ad161c3add67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a4de0c-3f54-4950-97bb-05b9f4c2f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Using cached scikit_image-0.24.0-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Collecting scipy>=1.9 (from scikit-image)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Collecting networkx>=2.8 (from scikit-image)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from scikit-image) (11.1.0)\n",
      "Collecting imageio>=2.33 (from scikit-image)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Using cached tifffile-2024.8.30-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\plawa\\anaconda3\\envs\\underwater_gan_new\\lib\\site-packages (from scikit-image) (24.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Using cached scikit_image-0.24.0-cp39-cp39-win_amd64.whl (12.9 MB)\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Using cached tifffile-2024.8.30-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: tifffile, scipy, networkx, lazy-loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.37.0 lazy-loader-0.4 networkx-3.2.1 scikit-image-0.24.0 scipy-1.13.1 tifffile-2024.8.30\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978179e1-0b10-4836-a0d9-5ed4783fc72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af2bb329-5c52-4d0b-86ac-70952d9310e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch: 1.12.1+cu113\n",
      "Torchvision: 0.13.1+cu113\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "#1: Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import DeformConv2d\n",
    "from torchmetrics.functional import peak_signal_noise_ratio, structural_similarity_index_measure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage.feature import canny\n",
    "import tqdm\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"CUDA:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c401c5b-7b8a-45d7-a34f-1c919ec07569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Edge Map Function\n",
    "def get_edge_map(image):\n",
    "    image_np = image.detach().cpu().numpy().transpose(1, 2, 0)  # Detach before converting to NumPy\n",
    "    image_np = (image_np + 1) / 2  # [-1, 1] to [0, 1]\n",
    "    edges = canny(image_np.mean(axis=2), sigma=2)  # Average RGB channels\n",
    "    return torch.tensor(edges, dtype=torch.float32).unsqueeze(0)  # [1, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5235939-6a57-44dd-871d-c0302321da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Enhanced Generator\n",
    "\n",
    "class DeformableBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.offset_conv = nn.Conv2d(in_channels, 2 * 3 * 3, 3, padding=1)\n",
    "        self.deform_conv = DeformConv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(16, out_channels)\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        offset = self.offset_conv(x)\n",
    "        x = self.deform_conv(x, offset)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(16, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(16, channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.leaky_relu(self.norm1(self.conv1(x)), 0.2)\n",
    "        x = self.norm2(self.conv2(x))\n",
    "        return x + residual\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.sigmoid(self.conv(x))\n",
    "        return x * attn\n",
    "\n",
    "class EnhancedGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder: Downsample to 64x64\n",
    "        self.enc1 = nn.Conv2d(4, 64, 4, 2, 1)  # 256x256 -> 128x128\n",
    "        self.enc2 = DeformableBlock(64, 128)    # 128x128\n",
    "        self.enc3 = nn.Conv2d(128, 256, 4, 2, 1)  # 128x128 -> 64x64\n",
    "        \n",
    "        # Residual blocks at bottleneck\n",
    "        self.res_blocks = nn.ModuleList([ResidualBlock(256) for _ in range(6)])\n",
    "        self.attn = AttentionModule(256)\n",
    "        \n",
    "        # Decoder: Upsample back to 256x256\n",
    "        self.dec3 = nn.ConvTranspose2d(256, 128, 4, 2, 1)  # 64x64 -> 128x128\n",
    "        self.dec2 = DeformableBlock(128, 64)               # 128x128\n",
    "        self.dec1 = nn.ConvTranspose2d(64, 3, 4, 2, 1)    # 128x128 -> 256x256\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = F.leaky_relu(self.enc1(x), 0.2)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = F.leaky_relu(self.enc3(e2), 0.2)\n",
    "        \n",
    "        r = e3\n",
    "        for block in self.res_blocks:\n",
    "            r = block(r)\n",
    "        r = self.attn(r)\n",
    "        \n",
    "        d3 = F.leaky_relu(self.dec3(r), 0.2)\n",
    "        d2 = self.dec2(d3)\n",
    "        d1 = self.dec1(d2)\n",
    "        \n",
    "        return self.tanh(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcdf8330-30cc-4306-9ea8-8d328972cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Multi-Scale Discriminator\n",
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scale1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.GroupNorm(16, 128), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, 4, 1, 0), nn.Sigmoid()\n",
    "        )\n",
    "        self.scale2 = nn.Sequential(\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.GroupNorm(16, 128), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, 4, 1, 0), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [self.scale1(x), self.scale2(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40fcf349-30a3-4471-8295-cc1408e0d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: Loss Functions\n",
    "def compute_content_loss(fake, real):\n",
    "    return F.l1_loss(fake, real)\n",
    "\n",
    "def compute_adversarial_loss(discriminator, fake, real):\n",
    "    real_out = discriminator(real)\n",
    "    fake_out = discriminator(fake)\n",
    "    loss = 0\n",
    "    for ro, fo in zip(real_out, fake_out):\n",
    "        loss += nn.BCELoss()(ro, torch.ones_like(ro)) + nn.BCELoss()(fo, torch.zeros_like(fo))\n",
    "    return loss / 2\n",
    "\n",
    "def compute_perceptual_loss(fake, real):\n",
    "    return F.mse_loss(fake, real)  # Simplified; use VGG if desired\n",
    "\n",
    "def compute_edge_loss(fake, real):\n",
    "    fake_edges = get_edge_map(fake)\n",
    "    real_edges = get_edge_map(real)\n",
    "    return F.l1_loss(fake_edges, real_edges)\n",
    "\n",
    "def compute_total_loss(generator, discriminator, fake, real):\n",
    "    c_loss = compute_content_loss(fake, real)\n",
    "    a_loss = compute_adversarial_loss(discriminator, fake, real)\n",
    "    p_loss = compute_perceptual_loss(fake, real)\n",
    "    e_loss = compute_edge_loss(fake, real)\n",
    "    lambda1, lambda2, lambda3, lambda4 = 1.0, 3.0, 0.01, 0.1\n",
    "    total_loss = (lambda1 * c_loss + lambda2 * a_loss + lambda3 * p_loss + lambda4 * e_loss)\n",
    "    return total_loss, c_loss, a_loss, p_loss, e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b5e6368-a4d0-474f-8f40-0d704441805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 11435\n",
      "Distorted shape: torch.Size([4, 256, 256]), Ground truth shape: torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#6: Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class EUVPDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Collect all trainA and trainB images from subdirectories\n",
    "        self.distorted_images = []\n",
    "        self.gt_images = []\n",
    "        \n",
    "        # Iterate through all subdirectories in Paired\n",
    "        for subdir in os.listdir(root_dir):\n",
    "            trainA_dir = os.path.join(root_dir, subdir, 'trainA')\n",
    "            trainB_dir = os.path.join(root_dir, subdir, 'trainB')\n",
    "            \n",
    "            if os.path.isdir(trainA_dir) and os.path.isdir(trainB_dir):\n",
    "                trainA_files = sorted(os.listdir(trainA_dir))\n",
    "                trainB_files = sorted(os.listdir(trainB_dir))\n",
    "                \n",
    "                # Ensure pairing matches\n",
    "                assert len(trainA_files) == len(trainB_files), f\"Mismatch in {subdir}: {len(trainA_files)} vs {len(trainB_files)}\"\n",
    "                \n",
    "                # Add full paths to lists\n",
    "                self.distorted_images.extend(os.path.join(trainA_dir, f) for f in trainA_files)\n",
    "                self.gt_images.extend(os.path.join(trainB_dir, f) for f in trainB_files)\n",
    "        \n",
    "        assert len(self.distorted_images) == len(self.gt_images), \"Total mismatch in image pairs\"\n",
    "        print(f\"Total images loaded: {len(self.distorted_images)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.distorted_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        distorted = Image.open(self.distorted_images[idx]).convert('RGB')\n",
    "        gt = Image.open(self.gt_images[idx]).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            distorted = self.transform(distorted)\n",
    "            gt = self.transform(gt)\n",
    "        \n",
    "        edge_map = get_edge_map(distorted).to(distorted.device)\n",
    "        distorted = torch.cat([distorted, edge_map], dim=0)  # [4, 256, 256]\n",
    "        return distorted, gt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Point to the Paired directory\n",
    "dataset = EUVPDataset(\n",
    "    r'C:\\Users\\plawa\\anaconda3\\envs\\underwater_gan_new\\EUVP\\Paired',\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Test loading\n",
    "distorted, gt = dataset[0]\n",
    "print(f\"Distorted shape: {distorted.shape}, Ground truth shape: {gt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8302c27-b7ed-4c8f-8ddc-e0a82f96e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]:   0%|                                                                                                                                      | 0/2859 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     39\u001b[0m fake \u001b[38;5;241m=\u001b[39m generator(distorted)\n\u001b[1;32m---> 40\u001b[0m g_loss, c_loss, a_loss, p_loss, e_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_total_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     42\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(generator\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 25\u001b[0m, in \u001b[0;36mcompute_total_loss\u001b[1;34m(generator, discriminator, fake, real)\u001b[0m\n\u001b[0;32m     23\u001b[0m a_loss \u001b[38;5;241m=\u001b[39m compute_adversarial_loss(discriminator, fake, real)\n\u001b[0;32m     24\u001b[0m p_loss \u001b[38;5;241m=\u001b[39m compute_perceptual_loss(fake, real)\n\u001b[1;32m---> 25\u001b[0m e_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_edge_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m lambda1, lambda2, lambda3, lambda4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     27\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m (lambda1 \u001b[38;5;241m*\u001b[39m c_loss \u001b[38;5;241m+\u001b[39m lambda2 \u001b[38;5;241m*\u001b[39m a_loss \u001b[38;5;241m+\u001b[39m lambda3 \u001b[38;5;241m*\u001b[39m p_loss \u001b[38;5;241m+\u001b[39m lambda4 \u001b[38;5;241m*\u001b[39m e_loss)\n",
      "Cell \u001b[1;32mIn[32], line 17\u001b[0m, in \u001b[0;36mcompute_edge_loss\u001b[1;34m(fake, real)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_edge_loss\u001b[39m(fake, real):\n\u001b[1;32m---> 17\u001b[0m     fake_edges \u001b[38;5;241m=\u001b[39m \u001b[43mget_edge_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     real_edges \u001b[38;5;241m=\u001b[39m get_edge_map(real)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39ml1_loss(fake_edges, real_edges)\n",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m, in \u001b[0;36mget_edge_map\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_edge_map\u001b[39m(image):\n\u001b[1;32m----> 3\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Detach before converting to NumPy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m (image_np \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# [-1, 1] to [0, 1]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     edges \u001b[38;5;241m=\u001b[39m canny(image_np\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Average RGB channels\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "#7: Training Loop\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Assuming EnhancedGenerator and MultiScaleDiscriminator are defined in Cells 3 and 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = EnhancedGenerator().to(device)\n",
    "discriminator = MultiScaleDiscriminator().to(device)\n",
    "\n",
    "# Optimizers with adjusted learning rates\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00002, betas=(0.5, 0.999))  # Slightly higher lr for discriminator\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Add tqdm for progress bar\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    for i, (distorted, ground_truth) in enumerate(train_loader_tqdm):\n",
    "        distorted, ground_truth = distorted.to(device), ground_truth.to(device)\n",
    "        \n",
    "        # Discriminator update (every 25 batches)\n",
    "        if i % 25 == 0:\n",
    "            d_optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                fake = generator(distorted)\n",
    "            real_noise = ground_truth + torch.randn_like(ground_truth) * 0.1  # Reduced noise intensity\n",
    "            fake_noise = fake.detach() + torch.randn_like(fake) * 0.1\n",
    "            d_loss = compute_adversarial_loss(discriminator, fake_noise, real_noise)\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            d_optimizer.step()\n",
    "        \n",
    "        # Generator update\n",
    "        g_optimizer.zero_grad()\n",
    "        fake = generator(distorted)\n",
    "        g_loss, c_loss, a_loss, p_loss, e_loss = compute_total_loss(generator, discriminator, fake, ground_truth)\n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Update progress bar with metrics every 10 batches\n",
    "        if i % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                psnr = peak_signal_noise_ratio(fake, ground_truth, data_range=2.0).item()\n",
    "                ssim = structural_similarity_index_measure(fake, ground_truth, data_range=2.0).item()\n",
    "            train_loader_tqdm.set_postfix({\n",
    "                'D Loss': f'{d_loss.item():.4f}',\n",
    "                'G Loss': f'{g_loss.item():.4f}',\n",
    "                'PSNR': f'{psnr:.2f}',\n",
    "                'SSIM': f'{ssim:.4f}'\n",
    "            })\n",
    "    \n",
    "    # Save models after each epoch\n",
    "    torch.save(generator.state_dict(), f\"generator_epoch_{epoch+1}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Saved models.\")\n",
    "\n",
    "# Final message\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf844122-dea8-4d79-9358-dff61c11a2bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      3\u001b[0m     distorted, ground_truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "#8 visualization\n",
    "\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    distorted, ground_truth = next(iter(train_loader))\n",
    "    distorted, ground_truth = distorted.to(device), ground_truth.to(device)\n",
    "    fake, _, _ = generator(distorted)\n",
    "    distorted_np = distorted[0, :3, :, :].cpu().numpy().transpose(1, 2, 0)\n",
    "    ground_truth_np = ground_truth[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    fake_np = fake[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    edge_np = distorted[0, 3, :, :].cpu().numpy()\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    axs[0].imshow((distorted_np + 1) / 2)\n",
    "    axs[0].set_title(\"Distorted\")\n",
    "    axs[1].imshow(edge_np, cmap='gray')\n",
    "    axs[1].set_title(\"Edge Map\")\n",
    "    axs[2].imshow((fake_np + 1) / 2)\n",
    "    axs[2].set_title(\"Restored\")\n",
    "    axs[3].imshow((ground_truth_np + 1) / 2)\n",
    "    axs[3].set_title(\"Ground Truth\")\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed4e02-6494-47c4-92b4-aefd37ba2e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "underwater_gan_new",
   "language": "python",
   "name": "underwater_gan_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
